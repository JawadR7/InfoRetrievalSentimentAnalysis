{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/JawadR7/InfoRetrievalSentimentAnalysis/raw/main/models_and_vectorizer.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zId-nfhAFgTZ",
        "outputId": "09d1a263-c685-44c4-9b82-8d304dac4cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-24 17:25:06--  https://github.com/JawadR7/InfoRetrievalSentimentAnalysis/raw/main/models_and_vectorizer.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/JawadR7/InfoRetrievalSentimentAnalysis/main/models_and_vectorizer.zip [following]\n",
            "--2024-04-24 17:25:06--  https://raw.githubusercontent.com/JawadR7/InfoRetrievalSentimentAnalysis/main/models_and_vectorizer.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11914999 (11M) [application/zip]\n",
            "Saving to: ‘models_and_vectorizer.zip’\n",
            "\n",
            "models_and_vectoriz 100%[===================>]  11.36M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-04-24 17:25:06 (173 MB/s) - ‘models_and_vectorizer.zip’ saved [11914999/11914999]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip models_and_vectorizer.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHYxOzLtGBmv",
        "outputId": "d0a051aa-b0ff-4898-e0c5-378ebee55681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  models_and_vectorizer.zip\n",
            "  inflating: CNB_model.sav           \n",
            "  inflating: regression_model.sav    \n",
            "  inflating: rfc_model.sav           \n",
            "  inflating: svm_model.sav           \n",
            "  inflating: tfidf_vectorizer.sav    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pickle\n",
        "\n",
        "class LemmaTokenizer:\n",
        "  ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
        "  def __init__(self):\n",
        "      self.wnl = WordNetLemmatizer()\n",
        "  def __call__(self, doc):\n",
        "      return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
        "\n",
        "tokenizer = LemmaTokenizer()\n",
        "\n",
        "def predict(x):\n",
        "  if x > 0.05:\n",
        "    return 'pos'\n",
        "  elif x < -0.05:\n",
        "    return 'neg'\n",
        "  return 'neu'\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Open models and TFIDF vectorizer from pickled .sav files\n",
        "vectorizer = pickle.load(open('tfidf_vectorizer.sav', 'rb'))\n",
        "CNB_demo = pickle.load(open('CNB_model.sav', 'rb'))\n",
        "reg_demo = pickle.load(open('regression_model.sav', 'rb'))\n",
        "svm_demo = pickle.load(open('svm_model.sav', 'rb'))\n",
        "rfc_demo = pickle.load(open('rfc_model.sav', 'rb'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn1z5qY4IEAr",
        "outputId": "fd6e3915-6608-4570-db61-775d24b355c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUc-mFQYFRfN",
        "outputId": "05134e9c-1ea5-43b4-fbeb-57fcd8532abb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type text and hit [ENTER]:\n",
            "I like to come here all the time!!!\n",
            "\n",
            "\n",
            "------------------------------------------------\n",
            "Predictions\n",
            "------------------------------------------------\n",
            "Complement Naive Bayes: pos\n",
            "Logistic Regression: pos\n",
            "SVM: pos\n",
            "Random Forest: pos\n",
            "VADER: pos\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DEMO\n",
        "# Input your own text (review) and see how each model classifies it\n",
        "\n",
        "# Take in user input an generate predictions if input is not null\n",
        "text = str(input(\"Type text and hit [ENTER]:\\n\"))\n",
        "\n",
        "if text == None:\n",
        "  print(\"ERROR: NO TEXT ENTERED\")\n",
        "else:\n",
        "  text_vector = vectorizer.transform([text])\n",
        "  CNB_result = CNB_demo.predict(text_vector)\n",
        "  reg_result = reg_demo.predict(text_vector)\n",
        "  svm_result = svm_demo.predict(text_vector)\n",
        "  rfc_result = rfc_demo.predict(text_vector)\n",
        "  vader_result = predict(sid.polarity_scores(text)['compound'])\n",
        "\n",
        "  print('\\n')\n",
        "  print('------------------------------------------------')\n",
        "  print(\"Predictions\")\n",
        "  print('------------------------------------------------')\n",
        "  print(f\"Complement Naive Bayes: {CNB_result[0]}\")\n",
        "  print(f\"Logistic Regression: {reg_result[0]}\")\n",
        "  print(f\"SVM: {svm_result[0]}\")\n",
        "  print(f\"Random Forest: {CNB_result[0]}\")\n",
        "  print(f\"VADER: {vader_result}\")\n",
        "  print('\\n\\n')"
      ]
    }
  ]
}